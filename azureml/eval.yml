$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
command: >
  python -m autora.doc.pipelines.main eval
  ${{inputs.data_dir}}/data.jsonl
  --model-path ${{inputs.model_path}}
  --sys-id ${{inputs.sys_id}}
  --instruc-id ${{inputs.instruc_id}}
  --param do_sample=${{inputs.do_sample}}
  --param temperature=${{inputs.temperature}}
  --param top_k=${{inputs.top_k}}
  --param top_p=${{inputs.top_p}}
code: ../src
inputs:
  data_dir:
    type: uri_folder
    path: azureml://datastores/workspaceblobstore/paths/data/sweetpea/
  # Currently models are loading faster directly from HuggingFace vs Azure Blob Storage
  # model_dir:
  #   type: uri_folder
  #   path: azureml://datastores/workspaceblobstore/paths/base_models
  model_path: meta-llama/Llama-2-7b-chat-hf
  temperature: 0.01
  do_sample: 0
  top_p: 0.95
  top_k: 1
  sys_id: SYS_1
  instruc_id: INSTR_SWEETP_1
# using a curated environment doesn't work because we need additional packages
environment: # azureml://registries/azureml/environments/acpt-pytorch-2.0-cuda11.7/versions/21
  image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:21
  # These didn't work
  # image: mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu117-py38-torch201:biweekly.202310.3
  # image: mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:latest
  # image: mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04
  # image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04
  # image: mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04
  # image: nvcr.io/nvidia/pytorch:23.10-py3
  conda_file: conda.yml
display_name: autodoc_prediction
compute: azureml:t4cluster
experiment_name: evaluation
description: |
